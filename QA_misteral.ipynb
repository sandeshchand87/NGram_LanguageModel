{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNiBOvEBL91LhSH8+W7UURe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2fc1101b88744cc6846108a7a4a3411b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a86d2b16e814ef6bfe0b1a9d8527771",
              "IPY_MODEL_433f28c5b77f4d1c99c0571e6450942c",
              "IPY_MODEL_7f05d5a2b96d4566bc09b950cb7c8311"
            ],
            "layout": "IPY_MODEL_24468d36af0d40e5adcaed5c52ef5674",
            "tabbable": null,
            "tooltip": null
          }
        },
        "4a86d2b16e814ef6bfe0b1a9d8527771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e1d41d5369cd4c65b39c834e91cc58bc",
            "placeholder": "​",
            "style": "IPY_MODEL_143a972fb7504a6ab8016a115ea0c65d",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "433f28c5b77f4d1c99c0571e6450942c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_21b253c2c4264c74be40434b565e8faa",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a191f1e6544471d8576d887fdbe9643",
            "tabbable": null,
            "tooltip": null,
            "value": 2
          }
        },
        "7f05d5a2b96d4566bc09b950cb7c8311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_946325af9a364ed886704c60efcba37c",
            "placeholder": "​",
            "style": "IPY_MODEL_0e7b746b75c74093970f852c5ef8e79c",
            "tabbable": null,
            "tooltip": null,
            "value": " 2/2 [00:10&lt;00:00,  4.81s/it]"
          }
        },
        "24468d36af0d40e5adcaed5c52ef5674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d41d5369cd4c65b39c834e91cc58bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "143a972fb7504a6ab8016a115ea0c65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "21b253c2c4264c74be40434b565e8faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a191f1e6544471d8576d887fdbe9643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "946325af9a364ed886704c60efcba37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7b746b75c74093970f852c5ef8e79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeshchand87/NGram_LanguageModel/blob/main/QA_misteral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/emburse/question-answering-over-documents-e92658e7a405"
      ],
      "metadata": {
        "id": "4X1US0cUQE4Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdAgYhqWN9h8",
        "outputId": "8a24fdcc-20fe-47ef-be2f-026da4fbee2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/286.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m276.5/286.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
            "  Downloading langchain_core-0.1.38-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.2/279.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.38-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting comm>=0.1.3 (from ipywidgets)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging>=20.0 (from transformers)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: widgetsnbextension, tzdata, pypdf, pyarrow, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, jsonpointer, jedi, einops, comm, typing-inspect, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, jsonpatch, nvidia-cusolver-cu12, langsmith, ipywidgets, dataclasses-json, transformers, torch, langchain-core, xformers, sentence_transformers, langchain-text-splitters, langchain-community, bitsandbytes, accelerate, langchain\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.6\n",
            "    Uninstalling widgetsnbextension-3.6.6:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.6\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.28.0 bitsandbytes-0.43.0 comm-0.2.2 dataclasses-json-0.6.4 einops-0.7.0 ipywidgets-8.1.2 jedi-0.19.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.38 langchain-text-splitters-0.0.1 langsmith-0.1.38 marshmallow-3.21.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 orjson-3.10.0 packaging-23.2 pandas-2.2.1 pyarrow-15.0.2 pypdf-4.1.0 sentence_transformers-2.6.1 torch-2.2.2 transformers-4.39.3 typing-inspect-0.9.0 tzdata-2024.1 widgetsnbextension-4.0.10 xformers-0.0.25.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pypdf torch transformers langchain ipywidgets accelerate \\\n",
        "  sentence_transformers pyarrow pandas bitsandbytes einops xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LC_ALL']='C.UTF-8'\n",
        "os.environ['LANG']='C.UTF-8'\n",
        "\n",
        "#drive.mount('/content/sample_data')\n",
        "folder_path = '/content/sample_data/data'\n",
        "print(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlDWSMRYONWH",
        "outputId": "6d56c67c-3ef2-46ee-872c-38890ad5e050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample_data/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "file = folder_path + \"/Surround View Camera system.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(folder_path + \"/Surround View Camera system.pdf\")\n",
        "pages= loader.load_and_split()"
      ],
      "metadata": {
        "id": "1Sf8DHM0ONOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap=50,\n",
        "    separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', '']\n",
        ")\n",
        "texts = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "z55LWlWiOoD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pages), len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5PCmrY-OoG5",
        "outputId": "b5687b03-f8d8-4b2d-8e4f-fba118c4d47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY8D3JE1OoKD",
        "outputId": "e9438c40-8370-4ea2-effa-1edee72081b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infront of the lens.  Generally, for a given sensor size, the shorter the focal length, th e wider the \n",
            "angle of view of the lens.  \n",
            " \n",
            " \n",
            "Figure 2 Distorted Image produced by fish eye camera  \n",
            "Camera calibration is the process of computing the extrinsic and intrinsic parameters of a \n",
            "camera. The extrinsic parameter such as  3*3 r otation matrix ,3*1 translation matrix and intrinsic \n",
            "parameters  are the camera parameter which is 3*3 matrix. The main goal is to find these \n",
            "parameters  and their corresponding image coordinates  (u, v). When we get the values of \n",
            "intrinsic and extrinsic parameters the camera is said to be calibrated.   \n",
            "Once a camera is calibrated, we can use the image information to recover 3 -D information \n",
            "from 2 -D images. we can also undistort images taken with a fisheye camera . \n",
            " \n",
            "Figure 3 Distorted Fisheye image and its undistorted image after correct3 \n",
            "The image coordinate system is the 2 -dimensional coordinate systems in pixel. So, the object \n",
            "has real world point (3 -dimensional coordinate system) which is required to relate to 2 -d \n",
            "image with the help of camera extrinsic and intrinsic parameters.  \n",
            " \n",
            "3 https://www.mathworks.com/help/vision/ug/fisheye -calibration -basics.html#mw_a53a2862 -986a -4026 -\n",
            "9011 -a5aba031bf97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding= HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "5nLZJgybP_Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import SKLearnVectorStore\n",
        "\n",
        "vector_db_path =  \"./document_vector_db.parquet\"\n",
        "vector_db = SKLearnVectorStore.from_documents(texts, embedding=embedding,persist_path= vector_db_path,\n",
        "                                              serializer=\"parquet\")\n"
      ],
      "metadata": {
        "id": "2nopLRDPP_Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is surroundview?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXWpXSeBP_a4",
        "outputId": "523c33c4-b7f1-4350-f4b4-3a8a52abed1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Surround View Camera -system  \\nIntroduction:  \\nThe Surround view system provides a complete 360-degree  view around the entire vehicles  \\n(car, ship etc) with the help of number of cameras  (fisheye cameras) mounted on it.  The number \\nof cameras is depending on the size and type of vehicles. Usually there are two types of \\ncameras - Pinhole cameras and fish eye cameras. In this system, we are going to discuss on fish', metadata={'id': '20eab42a-ef72-478f-91b3-0222c9f410a1', 'source': '/content/sample_data/data/Surround View Camera system.pdf', 'page': 0}), Document(page_content='projection matrix. This matrix will convert the undistorted or rectified camera picture into bird \\neye views of a rectangular area on the ground.  The projection matrices of these four cameras \\nare not independent, and they must ensure that the projected areas can be assembled exactly.  \\n \\nFigure 14 Parameter settings for bird eye view9 \\n \\n9 https://github.com/neozhaoliang/surround -view -system -introduction/blob/master/doc/doc.m d', metadata={'id': '597c3bc0-cfc2-44cf-8b0e-c961180a6249', 'source': '/content/sample_data/data/Surround View Camera system.pdf', 'page': 12}), Document(page_content='alignment corrects the fish eye distortion in the input video/ images frames and converts them \\nto a common bird’s  eye perspective. The composite surround view after the geometric \\ncorrection is generated by synthesis algorithm.  Similarly, there is also another key algorithm \\n“photometric alignment” which is responsible for producing stitched  (combining all the \\ncorrected input frames) sur round view output. Photometric alignment corrects the brightness', metadata={'id': '35fa209d-dc97-467f-8a90-2552febe0705', 'source': '/content/sample_data/data/Surround View Camera system.pdf', 'page': 0}), Document(page_content='Following steps can be used to obtain the final result of Bird’s eye view ( surround view ). \\n \\nFigure 12 Flowchart of Surround view system  \\n \\n1.Extraction of original image and camera internal parameter  \\nThe vehicles (car /ship)  have certain number of cameras (4 -6) installed on it. Suppose, there are \\nfour cameras installed which has given a name as front, back, left and right on the basis of their \\nposition. Our first work is to obtain the original imaged from e ach camera  and save in the', metadata={'id': 'bd745d4c-bcfa-432f-92b8-2c08c9a5eac1', 'source': '/content/sample_data/data/Surround View Camera system.pdf', 'page': 11})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "vector_db.persist()\n",
        "\n",
        "df = pd.read_parquet(vector_db_path)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0zYMUhSPRODC",
        "outputId": "d6205f74-ad76-45f1-9a21-2aaef4e1a73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     ids  \\\n",
              "0   20eab42a-ef72-478f-91b3-0222c9f410a1   \n",
              "1   564aeab1-0752-4c57-9b1d-57fd1efa495b   \n",
              "2   35fa209d-dc97-467f-8a90-2552febe0705   \n",
              "3   f1f7ecc2-80a1-4846-ae07-eb6dfd71628a   \n",
              "4   8156368d-7306-4a46-b03d-c52f0e97d29a   \n",
              "..                                   ...   \n",
              "65  f90ef2fc-67cc-4faa-a2a0-26f045aef904   \n",
              "66  ec4169f7-be3d-4d56-a97d-7daa9bc56fd0   \n",
              "67  74689c10-cc83-4b20-91ac-adf752f84c41   \n",
              "68  92c5d61b-3603-4ff1-a7cf-909ff767fffd   \n",
              "69  bab57ec2-f858-4fcb-af18-15bc7d55447a   \n",
              "\n",
              "                                                texts  \\\n",
              "0   Surround View Camera -system  \\nIntroduction: ...   \n",
              "1   eye cameras . As compare to Pinhole cameras, t...   \n",
              "2   alignment corrects the fish eye distortion in ...   \n",
              "3   and color mismatch between  adjacent  views  t...   \n",
              "4   but uses a special mapping  (equisolid angle),...   \n",
              "..                                                ...   \n",
              "65  pixel can easily detected. If the pixel falls ...   \n",
              "66  from 0 to 1.  The fused (merge )image can be o...   \n",
              "67  of different channel s (B, G, R) of the camera...   \n",
              "68  Figure 29 Final image after bright ness+ colou...   \n",
              "69  7. Adjusting the light and colour balance.  \\n...   \n",
              "\n",
              "                                            metadatas  \\\n",
              "0   {'page': 0, 'source': '/content/sample_data/da...   \n",
              "1   {'page': 0, 'source': '/content/sample_data/da...   \n",
              "2   {'page': 0, 'source': '/content/sample_data/da...   \n",
              "3   {'page': 0, 'source': '/content/sample_data/da...   \n",
              "4   {'page': 0, 'source': '/content/sample_data/da...   \n",
              "..                                                ...   \n",
              "65  {'page': 19, 'source': '/content/sample_data/d...   \n",
              "66  {'page': 20, 'source': '/content/sample_data/d...   \n",
              "67  {'page': 20, 'source': '/content/sample_data/d...   \n",
              "68  {'page': 21, 'source': '/content/sample_data/d...   \n",
              "69  {'page': 21, 'source': '/content/sample_data/d...   \n",
              "\n",
              "                                           embeddings  \n",
              "0   [-0.02578999288380146, -0.08342663943767548, 0...  \n",
              "1   [-0.0085429847240448, -0.027984950691461563, 0...  \n",
              "2   [-0.04039984196424484, -0.07470962405204773, 0...  \n",
              "3   [-0.012754177674651146, -0.08919082581996918, ...  \n",
              "4   [-0.006576374638825655, -0.10711628943681717, ...  \n",
              "..                                                ...  \n",
              "65  [-0.0328475758433342, -0.09125891327857971, -0...  \n",
              "66  [-0.02859026938676834, -0.10990551114082336, -...  \n",
              "67  [-0.00036798143992200494, -0.11691871285438538...  \n",
              "68  [0.019129816442728043, -0.0633750930428505, -0...  \n",
              "69  [-0.03223114833235741, -0.0797455906867981, 0....  \n",
              "\n",
              "[70 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e813323-012d-4a49-9e37-5e66af981ed1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ids</th>\n",
              "      <th>texts</th>\n",
              "      <th>metadatas</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20eab42a-ef72-478f-91b3-0222c9f410a1</td>\n",
              "      <td>Surround View Camera -system  \\nIntroduction: ...</td>\n",
              "      <td>{'page': 0, 'source': '/content/sample_data/da...</td>\n",
              "      <td>[-0.02578999288380146, -0.08342663943767548, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>564aeab1-0752-4c57-9b1d-57fd1efa495b</td>\n",
              "      <td>eye cameras . As compare to Pinhole cameras, t...</td>\n",
              "      <td>{'page': 0, 'source': '/content/sample_data/da...</td>\n",
              "      <td>[-0.0085429847240448, -0.027984950691461563, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35fa209d-dc97-467f-8a90-2552febe0705</td>\n",
              "      <td>alignment corrects the fish eye distortion in ...</td>\n",
              "      <td>{'page': 0, 'source': '/content/sample_data/da...</td>\n",
              "      <td>[-0.04039984196424484, -0.07470962405204773, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f1f7ecc2-80a1-4846-ae07-eb6dfd71628a</td>\n",
              "      <td>and color mismatch between  adjacent  views  t...</td>\n",
              "      <td>{'page': 0, 'source': '/content/sample_data/da...</td>\n",
              "      <td>[-0.012754177674651146, -0.08919082581996918, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8156368d-7306-4a46-b03d-c52f0e97d29a</td>\n",
              "      <td>but uses a special mapping  (equisolid angle),...</td>\n",
              "      <td>{'page': 0, 'source': '/content/sample_data/da...</td>\n",
              "      <td>[-0.006576374638825655, -0.10711628943681717, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>f90ef2fc-67cc-4faa-a2a0-26f045aef904</td>\n",
              "      <td>pixel can easily detected. If the pixel falls ...</td>\n",
              "      <td>{'page': 19, 'source': '/content/sample_data/d...</td>\n",
              "      <td>[-0.0328475758433342, -0.09125891327857971, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>ec4169f7-be3d-4d56-a97d-7daa9bc56fd0</td>\n",
              "      <td>from 0 to 1.  The fused (merge )image can be o...</td>\n",
              "      <td>{'page': 20, 'source': '/content/sample_data/d...</td>\n",
              "      <td>[-0.02859026938676834, -0.10990551114082336, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>74689c10-cc83-4b20-91ac-adf752f84c41</td>\n",
              "      <td>of different channel s (B, G, R) of the camera...</td>\n",
              "      <td>{'page': 20, 'source': '/content/sample_data/d...</td>\n",
              "      <td>[-0.00036798143992200494, -0.11691871285438538...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>92c5d61b-3603-4ff1-a7cf-909ff767fffd</td>\n",
              "      <td>Figure 29 Final image after bright ness+ colou...</td>\n",
              "      <td>{'page': 21, 'source': '/content/sample_data/d...</td>\n",
              "      <td>[0.019129816442728043, -0.0633750930428505, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>bab57ec2-f858-4fcb-af18-15bc7d55447a</td>\n",
              "      <td>7. Adjusting the light and colour balance.  \\n...</td>\n",
              "      <td>{'page': 21, 'source': '/content/sample_data/d...</td>\n",
              "      <td>[-0.03223114833235741, -0.0797455906867981, 0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e813323-012d-4a49-9e37-5e66af981ed1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e813323-012d-4a49-9e37-5e66af981ed1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e813323-012d-4a49-9e37-5e66af981ed1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-25409577-984b-4c93-9d5b-425f3d8de16d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25409577-984b-4c93-9d5b-425f3d8de16d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-25409577-984b-4c93-9d5b-425f3d8de16d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 70,\n  \"fields\": [\n    {\n      \"column\": \"ids\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 70,\n        \"samples\": [\n          \"e3ff48bd-e341-4751-84ba-1feb0fadbd40\",\n          \"20eab42a-ef72-478f-91b3-0222c9f410a1\",\n          \"58b745da-e65f-4b38-b37b-fc5976d18659\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"texts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 70,\n        \"samples\": [\n          \"localizing them.  \\n\\u2022 Corners of the squares are at the intersection of chessboard lines7. \\n \\nFigure 9 Ches sboard and its internal squares  \\nStep2.  Capture multiple images of the checkerboard from different viewpoints  \\n \\n6 https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html  \\n \\n7 https://learnopencv.com/camera -calibra tion-using -opencv/\",\n          \"Surround View Camera -system  \\nIntroduction:  \\nThe Surround view system provides a complete 360-degree  view around the entire vehicles  \\n(car, ship etc) with the help of number of cameras  (fisheye cameras) mounted on it.  The number \\nof cameras is depending on the size and type of vehicles. Usually there are two types of \\ncameras - Pinhole cameras and fish eye cameras. In this system, we are going to discuss on fish\",\n          \"some deformation of the distant object after being projected.  \\n\\u2022 totalWidth, totalHeight : Represents the total width and height of bird eye \\nview. Suppose, the calibration mat or cloth has dimension   6m*10m and let \\neach pixel corresponds to 1 cm. then  \\ntotalWidth = 600 + 2*shiftWidth  \\ntotalHeight = 600 + 2*shiftHeight  \\n\\u2022 four corners of the rectan gle where the vehicle is located ( red dots in figure14 ) \\nsuch as (xl, yt), (xr, yt), (xl, yb), (xr, yb) (where l = left, t=top, r=right,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadatas\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "tokenizer= AutoTokenizer.from_pretrained(model)\n",
        "'''\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "'''\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    trust_remote_code=True,\n",
        "    load_in_4bit=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "generate_text = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n",
        "                         trust_remote_code=True, max_new_tokens=100,\n",
        "                         repetition_penalty=1.1, model_kwargs={\"device_map\": \"auto\",\n",
        "                          \"max_length\": 1200, \"temperature\": 0.01, \"torch_dtype\":torch.bfloat16}\n",
        ")\n",
        "'''\n",
        "generate_text = pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.7,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")\n",
        "'''\n",
        "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
        "print(\"model loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "2fc1101b88744cc6846108a7a4a3411b",
            "4a86d2b16e814ef6bfe0b1a9d8527771",
            "433f28c5b77f4d1c99c0571e6450942c",
            "7f05d5a2b96d4566bc09b950cb7c8311",
            "24468d36af0d40e5adcaed5c52ef5674",
            "e1d41d5369cd4c65b39c834e91cc58bc",
            "143a972fb7504a6ab8016a115ea0c65d",
            "21b253c2c4264c74be40434b565e8faa",
            "3a191f1e6544471d8576d887fdbe9643",
            "946325af9a364ed886704c60efcba37c",
            "0e7b746b75c74093970f852c5ef8e79c"
          ]
        },
        "id": "IurM59vgSzF8",
        "outputId": "ac871958-4ad9-4f28-ce22-f819c9fc99e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fc1101b88744cc6846108a7a4a3411b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=hf_pipeline, chain_type=\"stuff\",\n",
        "                                 retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                 return_source_documents=True,\n",
        "                                 verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "C3P7x6y2dziU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pjmj9ENgILi",
        "outputId": "8630c9de-b288-46c2-eea5-dd44c49f5f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7d829a6bab90>)), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['SKLearnVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.sklearn.SKLearnVectorStore object at 0x7d830dfc3ac0>, search_kwargs={'k': 3}))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"tell me the hardware used in Surroundview project \"\n",
        "result = qa({\"query\":query})\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14fJzaaNnpuD",
        "outputId": "e9618903-4250-45ca-e41d-7725054235c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Surround View Camera -system  \n",
            "Introduction:  \n",
            "The Surround view system provides a complete 360-degree  view around the entire vehicles  \n",
            "(car, ship etc) with the help of number of cameras  (fisheye cameras) mounted on it.  The number \n",
            "of cameras is depending on the size and type of vehicles. Usually there are two types of \n",
            "cameras - Pinhole cameras and fish eye cameras. In this system, we are going to discuss on fish\n",
            "\n",
            "time . \n",
            "We have two parts in this documentation 1. Camera calibration (whi ch we already explained) \n",
            "2. Perspective transformation.  \n",
            "To implement this project on production environment, the proper hardware is used. We can use \n",
            "hardware like AGX Xavier  etc. As hardware selection process is in research phase so, we have \n",
            "not included i n this document.\n",
            "\n",
            "Following steps can be used to obtain the final result of Bird’s eye view ( surround view ). \n",
            " \n",
            "Figure 12 Flowchart of Surround view system  \n",
            " \n",
            "1.Extraction of original image and camera internal parameter  \n",
            "The vehicles (car /ship)  have certain number of cameras (4 -6) installed on it. Suppose, there are \n",
            "four cameras installed which has given a name as front, back, left and right on the basis of their \n",
            "position. Our first work is to obtain the original imaged from e ach camera  and save in the\n",
            "\n",
            "Question: tell me the hardware used in Surroundview project \n",
            "Helpful Answer:\n",
            "\n",
            "The hardware used in the Surroundview project is the AGX Xavier. It is a computer vision system that can be used to capture and process images in real-time. The AGX Xavier is a powerful system that can be used to capture and process images in real-time. It can be used to capture images from multiple cameras and can be used to create a 360-degree view of the surrounding environment. The AGX Xavier can also be used to perform object recognition and tracking,\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To implement this project on production environment, the proper hardware is used\n",
        "\n",
        "query =  \"which hardware is  used  in Surroundview project on production environment \"\n",
        "result = qa({\"query\":query})\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJk6lLbBoi_D",
        "outputId": "2afffbf6-2243-475b-da1c-5fd4a453ff40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "time . \n",
            "We have two parts in this documentation 1. Camera calibration (whi ch we already explained) \n",
            "2. Perspective transformation.  \n",
            "To implement this project on production environment, the proper hardware is used. We can use \n",
            "hardware like AGX Xavier  etc. As hardware selection process is in research phase so, we have \n",
            "not included i n this document.\n",
            "\n",
            "Surround View Camera -system  \n",
            "Introduction:  \n",
            "The Surround view system provides a complete 360-degree  view around the entire vehicles  \n",
            "(car, ship etc) with the help of number of cameras  (fisheye cameras) mounted on it.  The number \n",
            "of cameras is depending on the size and type of vehicles. Usually there are two types of \n",
            "cameras - Pinhole cameras and fish eye cameras. In this system, we are going to discuss on fish\n",
            "\n",
            "Following steps can be used to obtain the final result of Bird’s eye view ( surround view ). \n",
            " \n",
            "Figure 12 Flowchart of Surround view system  \n",
            " \n",
            "1.Extraction of original image and camera internal parameter  \n",
            "The vehicles (car /ship)  have certain number of cameras (4 -6) installed on it. Suppose, there are \n",
            "four cameras installed which has given a name as front, back, left and right on the basis of their \n",
            "position. Our first work is to obtain the original imaged from e ach camera  and save in the\n",
            "\n",
            "Question: which hardware is  used  in Surroundview project on production environment \n",
            "Helpful Answer:\n",
            "\n",
            "The hardware used in the Surround View project on production environment is the AGX Xavier system. The AGX Xavier system is a multi-camera system that can be used to capture 360-degree images. It is a powerful system that can be used to capture images from multiple angles and angles of view. The system is designed to work with multiple cameras, which can be used to capture images from different angles of the same object. The system can be used to capture images from a variety of\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"what are the parameters require for camera calibration process?\"\n",
        "\n",
        "result = qa({\"query\": query})\n",
        "\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSV3qq-IeZ2j",
        "outputId": "3e8f6aa7-f28d-4d2b-8bd8-1d6c20aa13f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "During the calibration process, some parameters are discovered to fix most of the distortions \n",
            "caused by the camera . \n",
            "• Extrinsic parameters:  rotation and transl ation vectors that translate a 3 D points to 2 \n",
            "D coordinates  \n",
            "• Intrinsic parameters:  focal length, optical centre  and skew coefficients which is \n",
            "specific for each camera.  \n",
            "• Radial distortion coefficients : causes straight line to appear curved. It occurs when\n",
            "\n",
            "For camera  calibration , we can use calibration board (chessboard ) by holding it in our hand and \n",
            "take many images Infront of the camera. Same process will apply for each of the camera. The \n",
            "camera internal parameters and distortion coefficient can easily be obtained  by camer a \n",
            "calibration method which is discussed in camera calibration section above  (Steps for camera \n",
            "calibration and Undistortion ). Once the required parameter from each camera obtained, it\n",
            "\n",
            "Infront of the lens.  Generally, for a given sensor size, the shorter the focal length, th e wider the \n",
            "angle of view of the lens.  \n",
            " \n",
            " \n",
            "Figure 2 Distorted Image produced by fish eye camera  \n",
            "Camera calibration is the process of computing the extrinsic and intrinsic parameters of a \n",
            "camera. The extrinsic parameter such as  3*3 r otation matrix ,3*1 translation matrix and intrinsic \n",
            "parameters  are the camera parameter which is 3*3 matrix. The main goal is to find these\n",
            "\n",
            "Question: what are the parameters require for camera calibration process?\n",
            "Helpful Answer:\n",
            "\n",
            "The parameters required for camera calibration process are:\n",
            "\n",
            "1. Extrinsic parameters: rotation and translation vectors that translate a 3D point to 2D coordinates.\n",
            "\n",
            "2. Intrinsic parameters: focal length, optical center, and skew coefficients which are specific for each camera.\n",
            "\n",
            "3. Radial distortion coefficients: causes straight lines to appear curved. It occurs when holding a calibration board in front of the camera.\n",
            "\n",
            "4. Camera calibration method:\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"Define real world coordinate system with chess board patterns.\"\n",
        "result = qa({\"query\":query})\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1a3pp2FeZ57",
        "outputId": "0a8aa2a3-3bfe-4e9a-c057-e0bd8501a459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "world coordinate is attached to the che ssboard and since all the corner points lie on a plane, we \n",
            "can arbitrarily choose Zw for every point to be 0. Since points are equally spaced in the \n",
            "chessboard, the (Xw, Yw) coordinates of each 3D point are easily de fined by taking one point \n",
            "as reference (0, 0) and defining remaining with respect to that reference point.  For X w, Yw \n",
            "values, we can simply pass the points as (0,0), (1,0), (2,0), ... which denotes the location of\n",
            "\n",
            "we get the results in mm. (In this case, we don't know square size since we didn't take those \n",
            "images, so we pass in terms of square size)6. \n",
            "3D points are called  object points  and 2D image points are called  image points.  \n",
            " \n",
            "Figure 8 real posit ion of chessboard  \n",
            "Why is the chess board pattern so widely used in calibration?  \n",
            "• Patterns are distinct and easy to detect an image.  \n",
            "• Corners of the square have sharp gradients in two direction and are ideal for \n",
            "localizing them.\n",
            "\n",
            "Note : \n",
            "This function may not be able to find the required pattern in all the images. So, one good option \n",
            "is to write the code such that, it starts the camera and check each frame for required pattern. \n",
            "Once the pattern is obtained, find the corners and store it in  a list. Also, provide some interval \n",
            "before reading next frame so that we can adjust our chess board in different direction. Continue \n",
            "this process until the required number of good patterns are obtained. Even in the real-world\n",
            "\n",
            "Question: Define real world coordinate system with chess board patterns.\n",
            "Helpful Answer:\n",
            "\n",
            "The real world coordinate system with chess board patterns is as follows:\n",
            "\n",
            "1. The chess board is divided into 64 squares, each of which is 64 pixels in size.\n",
            "2. Each square is divided into 64 smaller squares, each of which is 32 pixels in size.\n",
            "3. The chess board is divided into 8 x 8 x 8 x 8 x 8 x 8 x 8 x 8 x 8 x 8\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"Why is the chessboard pattern so widely used in calibration?\"\n",
        "result = qa({\"query\":query})\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3-zDmtJSzKC",
        "outputId": "3ffeb696-aa79-4b3e-ceba-30c6431d2cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "we get the results in mm. (In this case, we don't know square size since we didn't take those \n",
            "images, so we pass in terms of square size)6. \n",
            "3D points are called  object points  and 2D image points are called  image points.  \n",
            " \n",
            "Figure 8 real posit ion of chessboard  \n",
            "Why is the chess board pattern so widely used in calibration?  \n",
            "• Patterns are distinct and easy to detect an image.  \n",
            "• Corners of the square have sharp gradients in two direction and are ideal for \n",
            "localizing them.  \n",
            "• Corners of the squares are at the intersection of chessboard lines7. \n",
            " \n",
            "Figure 9 Ches sboard and its internal squares  \n",
            "Step2.  Capture multiple images of the checkerboard from different viewpoints  \n",
            " \n",
            "6 https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html  \n",
            " \n",
            "7 https://learnopencv.com/camera -calibra tion-using -opencv/\n",
            "\n",
            "Note : \n",
            "This function may not be able to find the required pattern in all the images. So, one good option \n",
            "is to write the code such that, it starts the camera and check each frame for required pattern. \n",
            "Once the pattern is obtained, find the corners and store it in  a list. Also, provide some interval \n",
            "before reading next frame so that we can adjust our chess board in different direction. Continue \n",
            "this process until the required number of good patterns are obtained. Even in the real-world \n",
            "scenario , we are not sure how many images out of the tested images  are good. Thus, we must \n",
            "read all the images and take only the good ones.  \n",
            "Instead of chess board, we can alternatively use a circular grid. In this case, we must use the \n",
            "function cv.findCirclesGrid( ) to find the pattern. Fewer images are sufficient to perform camera \n",
            "calibration using a circular grid.8 \n",
            "Refine the checkboard corners  \n",
            "Good calibration is all about precision. To get good results it is important to obtain the location\n",
            "\n",
            "Question: Why is the chessboard pattern so widely used in calibration?\n",
            "Helpful Answer:\n",
            "\n",
            "The chessboard pattern is widely used in calibration because it is distinct and easy to detect an image. Corners of the square have sharp gradients in two directions and are ideal for localizing them. Corners of the squares are at the intersection of chessboard lines. Additionally, the chessboard pattern is a common pattern in the real world, so it is easy to find it in a variety of images.\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmFpZtFSSzNP",
        "outputId": "90331c48-29dd-4b92-dc19-094ae7ae73bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Why is the chessboard pattern so widely used in calibration?',\n",
              " 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nwe get the results in mm. (In this case, we don't know square size since we didn't take those \\nimages, so we pass in terms of square size)6. \\n3D points are called  object points  and 2D image points are called  image points.  \\n \\nFigure 8 real posit ion of chessboard  \\nWhy is the chess board pattern so widely used in calibration?  \\n• Patterns are distinct and easy to detect an image.  \\n• Corners of the square have sharp gradients in two direction and are ideal for \\nlocalizing them.  \\n• Corners of the squares are at the intersection of chessboard lines7. \\n \\nFigure 9 Ches sboard and its internal squares  \\nStep2.  Capture multiple images of the checkerboard from different viewpoints  \\n \\n6 https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html  \\n \\n7 https://learnopencv.com/camera -calibra tion-using -opencv/\\n\\nNote : \\nThis function may not be able to find the required pattern in all the images. So, one good option \\nis to write the code such that, it starts the camera and check each frame for required pattern. \\nOnce the pattern is obtained, find the corners and store it in  a list. Also, provide some interval \\nbefore reading next frame so that we can adjust our chess board in different direction. Continue \\nthis process until the required number of good patterns are obtained. Even in the real-world \\nscenario , we are not sure how many images out of the tested images  are good. Thus, we must \\nread all the images and take only the good ones.  \\nInstead of chess board, we can alternatively use a circular grid. In this case, we must use the \\nfunction cv.findCirclesGrid( ) to find the pattern. Fewer images are sufficient to perform camera \\ncalibration using a circular grid.8 \\nRefine the checkboard corners  \\nGood calibration is all about precision. To get good results it is important to obtain the location\\n\\nQuestion: Why is the chessboard pattern so widely used in calibration?\\nHelpful Answer:\\n\\nThe chessboard pattern is widely used in calibration because it is distinct and easy to detect an image. Corners of the square have sharp gradients in two directions and are ideal for localizing them. Corners of the squares are at the intersection of chessboard lines. Additionally, the chessboard pattern is a common pattern in the real world, so it is easy to find it in a variety of images.\",\n",
              " 'source_documents': [Document(page_content=\"we get the results in mm. (In this case, we don't know square size since we didn't take those \\nimages, so we pass in terms of square size)6. \\n3D points are called  object points  and 2D image points are called  image points.  \\n \\nFigure 8 real posit ion of chessboard  \\nWhy is the chess board pattern so widely used in calibration?  \\n• Patterns are distinct and easy to detect an image.  \\n• Corners of the square have sharp gradients in two direction and are ideal for \\nlocalizing them.  \\n• Corners of the squares are at the intersection of chessboard lines7. \\n \\nFigure 9 Ches sboard and its internal squares  \\nStep2.  Capture multiple images of the checkerboard from different viewpoints  \\n \\n6 https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html  \\n \\n7 https://learnopencv.com/camera -calibra tion-using -opencv/\", metadata={'id': '6bcd8172-0cf6-42d3-b54d-ff785df90951', 'source': '/content/sample_data/data/Surround View Camera system.pdf', 'page': 6}),\n",
              "  Document(page_content='Note : \\nThis function may not be able to find the required pattern in all the images. So, one good option \\nis to write the code such that, it starts the camera and check each frame for required pattern. \\nOnce the pattern is obtained, find the corners and store it in  a list. Also, provide some interval \\nbefore reading next frame so that we can adjust our chess board in different direction. Continue \\nthis process until the required number of good patterns are obtained. Even in the real-world \\nscenario , we are not sure how many images out of the tested images  are good. Thus, we must \\nread all the images and take only the good ones.  \\nInstead of chess board, we can alternatively use a circular grid. In this case, we must use the \\nfunction cv.findCirclesGrid( ) to find the pattern. Fewer images are sufficient to perform camera \\ncalibration using a circular grid.8 \\nRefine the checkboard corners  \\nGood calibration is all about precision. To get good results it is important to obtain the location', metadata={'id': 'c19bb82e-ac07-4e0f-9839-32507161f260', 'source': '/content/sample_data/data/Surround View Camera system.pdf', 'page': 7})]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"how to calcualte each pixel of images in overlapping area.\"\n",
        "result = qa({\"query\":query})\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "062488QpROGU",
        "outputId": "36ee99d0-4645-421d-db58-7074a0a3a193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Figure 25 Overlapping image after morphol ogical operation  \n",
            "4. Find the  outer boundar ies of left  and front images lies outside the overlapping area projection \n",
            "image using  cv2.findcontour()  and cv.appro xPolyD P (for appro ximate polygon  outline ) \n",
            " \n",
            "Figure 26 Images o f non-overlapping  area (Front , left Image ) \n",
            "polyA  (the blue border in the upper left image)  is obtained by the contour after subtracting the \n",
            "overlapping area from the front camera  while polyB  (the green border in the upper right ima ge) \n",
            "is obtained after subtracting the overlapping area from the left camera . \n",
            " \n",
            "4.  Calculate the w eight of each pi xel in the overlapping area . First calculate  the distance  (dA, \n",
            "dB) of each pixel  to polygon  polyA and  polyB using cv2.pointPolygonTest . the corresponding \n",
            "weight of each pi xel can be calculate d as w = db ²/(dA²+dB² ). From this metho d, the location of \n",
            "pixel can easily detected. If the pixel falls within the front picture  (i.e near to polyA ), it is far\n",
            "\n",
            "In Figure 22.  The projection result of the ad jacent camera in the overlappi ng area are not \n",
            "completely  consistent  due to the error  of correction  and projection  and the sti tching re sults has \n",
            "some garbled character.  To correct ing this issue,  the w eightage coefficient  should change wit h \n",
            "pixel continuously . \n",
            "2.Find the overlapping area of the two adjacent cameras  (Front -left) field of view  from the \n",
            "projection map.  Perform grayscale conversion, binarization a nd remove no ise using \n",
            "morphological  operation  if present in the pixe ls and obtain final mask of overlapping area in \n",
            "black and white image  \n",
            " \n",
            "Figure 23 overlapping area in the upper left part of projection ma p \n",
            " \n",
            "Figure 24 binary image of overlapping area\n",
            "\n",
            "Question: how to calcualte each pixel of images in overlapping area.\n",
            "Helpful Answer:\n",
            "\n",
            "To calculate the width of each pixel in the overlapping area, you can use the following formula:\n",
            "\n",
            "```python\n",
            "width = (polyA.x - polyB.x) / (polyA.y - polyB.y)\n",
            "```\n",
            "\n",
            "This formula subtracts the coordinates of the two polygons from each other, and then divides the result by the difference in their y-coordinates.\n",
            "\n",
            "To calculate the distance between each pixel in the overlapping area, you can use the following formula:\n",
            "\n",
            "```python\n",
            "distance = sqrt((polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (polyA.x - polyB.x) * (polyA.y - polyB.y) + (poly\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =  \"how to obtain the merge image of two  front and left image.\"\n",
        "result = qa({\"query\":query})\n",
        "print(\"Response :\", result[\"result\"]  )\n",
        "print(\"source:\",result['source_documents'][0].metadata['source'])\n",
        "print(\"page:\",int(result['source_documents'][0].metadata['page']+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfqOMLarROJ5",
        "outputId": "aaf2c194-14aa-40c0-964f-d4cbab66d839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response : Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "from 0 to 1.  The fused (merge )image can be obt ained as front image * G + ( 1-G) *left image \n",
            "where G is the weight matrix . \n",
            "6. Adjust the light balance  of each area so that the brightness in the stitched image tends to b e \n",
            "consi stent.  Different camera has different  light exposure due to brightness difference  between  \n",
            "light and dark in different areas and affects the appearance . \n",
            "7. Adjust the colour  balance of the stitc hed image  which is caused by the different intensiti es \n",
            "of different channel s (B, G, R) of the camera.  \n",
            "                     \n",
            "Figure 27 Final image  after stitching  \n",
            " \n",
            "Figure 28 Final image after brig htness bala nce\n",
            "\n",
            "4.Stitching and smoothing of bird’s eye view  \n",
            "This is the final and most important step to sti tch(merge ) all the images obtained from each \n",
            "camera to achieve bird ’s eye view  \n",
            "The final image of bird eye view as follows:  \n",
            " \n",
            "Figure 21 Bird's eye view  (surround view ) \n",
            "There are some steps to follow to achieve  the final goal.  \n",
            "1.  The overlapping area between adjacent cameras as discussed ab ove need to be fused which \n",
            "is the key for the stitching the images.  To achieve this work, the weighted average matrix of \n",
            "two images should be determin ed that combined two images smoothly . The final result would \n",
            "look like this  \n",
            " \n",
            "Figure 22 bird eye view obtained by directly taking the weighted average between t wo \n",
            "images.\n",
            "\n",
            "Question: how to obtain the merge image of two  front and left image.\n",
            "Helpful Answer:\n",
            "\n",
            "To obtain the merge image of two front and left images, you can use the following steps:\n",
            "\n",
            "1. Take the weighted average of the two images. This can be done using the following formula:\n",
            "\n",
            "```\n",
            "G = (A + B) / (A + B + C)\n",
            "```\n",
            "\n",
            "where A, B, and C are the pixel values of the two images.\n",
            "\n",
            "2. Apply the weighted average to the left and front images. This can be done using the following formulas:\n",
            "\n",
            "```\n",
            "L = (A + B) / (A + B + C)\n",
            "F = (A + B) / (A + B + C)\n",
            "```\n",
            "\n",
            "where L and F are the pixel values of the left and front images, respectively.\n",
            "\n",
            "3. Apply the weighted average to the left and front images again. This can be done using the following formulas:\n",
            "\n",
            "```\n",
            "L = (A + B) / (A + B + C)\n",
            "F = (A + B) / (A + B + C)\n",
            "```\n",
            "\n",
            "where L and F are the pixel values of the left and front images, respectively.\n",
            "\n",
            "4. Apply the weighted average to the left and front images again. This can be done using the following formulas:\n",
            "\n",
            "```\n",
            "L = (A + B) / (A + B + C)\n",
            "F = (A + B) / (A + B + C)\n",
            "```\n",
            "\n",
            "where L and F are the pixel values of the left and front images, respectively.\n",
            "\n",
            "5. Apply the weighted average to the left and front images again. This can be done using the following formulas:\n",
            "\n",
            "```\n",
            "L = (A + B) / (A + B + C)\n",
            "F = (A + B) / (A + B + C)\n",
            "```\n",
            "\n",
            "where L and F are the pixel values of the left and front images, respectively.\n",
            "\n",
            "6. Apply the weighted average to the left and front images again. This can be done using the following formulas:\n",
            "\n",
            "```\n",
            "L = (A + B) / (A + B + C\n",
            "source: /content/sample_data/data/Surround View Camera system.pdf\n",
            "page: 21\n"
          ]
        }
      ]
    }
  ]
}